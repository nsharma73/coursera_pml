---
title: "Weight Lifting Exercise Classification"
author: "Nimit Sharma"
date: "4/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Practical Machine Learning Course - Project

The project uses exercise data from the following paper by:
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz6rmStrvFC

My name is Nimit Sharma and I used h2o library to build two classification models: 1) Random Forest and 2) Gradient Boosting Machine (GBM)

```{r, echo=FALSE}
# Initialize the h2o server and clear any existing objects
library(h2o)
h2o.init()
h2o.removeAll()
h2o.no_progress()
# set working directory
setwd("~/Desktop/ML Course/Coursera/PML/Project")
```

The pml-training.csv file was used to train several models. 

```{r}
# Load the dataset
dataset = read.csv('pml-training.csv')
df.hex <- as.h2o(dataset)
# head(df.hex)

# the classe variable should be a factor variable
df.hex['classe'] <- as.factor(df.hex['classe'])
```

Prior to building data we split the data in training, validation and test datasets. The first few columns were not used as features, as these data represent time of the test and the person or subject lifting weights.

```{r}
# Split the data into train, test and validation samples
# We split the data into three pieces: 60% for training, 20% for validation, 20% for final testing. 
# Here, we use random splitting, but this assumes i.i.d. data. 

parts<-h2o.splitFrame(df.hex, c(0.6,0.2), seed = -1)
train <- parts[[1]]
valid <- parts[[2]]
test <- parts[[3]]
x<-colnames(train, do.NULL = FALSE)[8:159]
y<-colnames(train, do.NULL = FALSE)[160]
```


Let's build the random forest model first

```{r}
m<-h2o.randomForest(x, y, train, nfolds=7)
summary(m)
h2o.varimp(m)
h2o.confusionMatrix(m,test)
# error 0.0087
h2o.performance(m,test)
```

The random forest error on test dataset was 0.87% next we try the GBM model. However, we construct a grid first to tune the hyperparameters.

```{r}
g1<- h2o.grid("gbm", grid_id = "GBM_stew",
              search_criteria = list(
                strategy = "RandomDiscrete",
                max_models = 20 # make 20 models and then stop
              ),
              hyper_params = list(
                max_depth = c(5,10,15), # depth of trees
                min_rows = c(2,5,10),
                sample_rate = c(0.6,0.8,1.0), # sample 60%, 80% and 100% of rows per tree
                col_sample_rate = c(0.5,0.8,1.0), # sample 50%, 80% and 100% of columns per split
                col_sample_rate_per_tree = c(0.8,1.0), # search a large space of column sampling 
                                                       # rates per tree
                seed = 12345 ## fix a random number generator seed for reproducibility
              ),
              x = x, y = y, training_frame = train, validation_frame = valid,
              
              stopping_tolerance = 0.001, # requires at least 0.01% improvement in a give metric
              stopping_rounds = 3, # if none of the last 3 random models have managed to be 0.01% 
                                   # better than the best random model before that, then stop 
              score_tree_interval = 10, # score every 10 trees to make early stopping reproducible 
                                        # (it depends on the scoring interval)
              ntrees = 400
              )
```

We now study the performance of models generated by grid search:

```{r}
# Sort by accuracy metric
gbm_gridperf1 <- h2o.getGrid(grid_id = "GBM_stew",
                             sort_by = "accuracy",
                             decreasing = TRUE)
print(gbm_gridperf1)
# Sort by logloss metric
gbm_gridperf2 <- h2o.getGrid(grid_id = "GBM_stew",
                               sort_by = "logloss",
                               decreasing = FALSE)
print(gbm_gridperf2)
# Sort by MSE metric
gbm_gridperf3 <- h2o.getGrid(grid_id = "GBM_stew",
                             sort_by = "MSE",
                             decreasing = FALSE)
print(gbm_gridperf3)
# Sort by RMSE metric
gbm_gridperf4 <- h2o.getGrid(grid_id = "GBM_stew",
                             sort_by = "rmse",
                             decreasing = FALSE)
print(gbm_gridperf4)
```

We determine which model is the best model as sorted by h2o

```{r}
m_gbm <- h2o.getModel(g1@model_ids[[1]])
```

But did we outperform the random forest?

```{r}
h2o.varimp(m_gbm)
h2o.confusionMatrix(m_gbm,test)
h2o.performance(m_gbm,test)
# error is 0.004
```

The answer is yes, we went from 0.87% to 0.4% error

## Including Plots

We can see variable importance and partial dependency plots

```{r, echo=FALSE}
h2o.varimp_plot(m_gbm)
h2o.pd_plot(m_gbm, column = 'roll_belt', newdata = test)

h2o.partialPlot(object = m_gbm, data = test, cols = "roll_belt", 
                targets=c("A", "B", "C", "D", "E"))
```

## Let's Predict - The final step

```{r}
# Load the test data
dataset_test = read.csv('pml-testing.csv')
df_test.hex <- as.h2o(dataset_test)

# Predict using the GBM model and the testing dataset
pred <- h2o.predict(object = m_gbm, newdata = df_test.hex[8:159])
dim(pred)
# Export data to a CSV file
h2o.exportFile(pred, "pred.csv", force = TRUE)
```



